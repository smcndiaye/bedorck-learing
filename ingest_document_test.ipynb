{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>Jupyter.notebook.kernel.restart()</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# restart kernel\n",
    "from IPython.core.display import HTML\n",
    "HTML(\"<script>Jupyter.notebook.kernel.restart()</script>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "import pprint\n",
    "from utility import create_bedrock_execution_role, create_oss_policy_attach_bedrock_execution_role, create_policies_in_oss, interactive_sleep\n",
    "import random\n",
    "from retrying import retry "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "suffix = random.randrange(200, 900)\n",
    "\n",
    "sts_client = boto3.client('sts')\n",
    "boto3_session = boto3.session.Session()\n",
    "region_name = boto3_session.region_name\n",
    "bedrock_agent_client = boto3_session.client('bedrock-agent', region_name=region_name)\n",
    "service = 'aoss'\n",
    "s3_client = boto3.client('s3')\n",
    "account_id = sts_client.get_caller_identity()[\"Account\"]\n",
    "s3_suffix = f\"{region_name}-{account_id}\"\n",
    "bucket_name = f'bedrock-kb-{s3_suffix}' # replace it with your bucket name.\n",
    "pp = pprint.PrettyPrinter(indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket bedrock-kb-us-east-1-266735831348 Exists\n"
     ]
    }
   ],
   "source": [
    "# Check if bucket exists, and if not create S3 bucket for knowledge base data source\n",
    "try:\n",
    "    s3_client.head_bucket(Bucket=bucket_name)\n",
    "    print(f'Bucket {bucket_name} Exists')\n",
    "except ClientError as e:\n",
    "    print(f'Creating bucket {bucket_name}')\n",
    "    if region_name == \"us-east-1\":\n",
    "        s3bucket = s3_client.create_bucket(\n",
    "            Bucket=bucket_name)\n",
    "    else:\n",
    "        s3bucket = s3_client.create_bucket(\n",
    "        Bucket=bucket_name,\n",
    "        CreateBucketConfiguration={ 'LocationConstraint': region_name }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'bucket_name' (str)\n"
     ]
    }
   ],
   "source": [
    "%store bucket_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a vector store - OpenSearch Serverless index\n",
    "\n",
    "### Step 1 - Create OSS policies and collection\n",
    "First of all we have to create a vector store. In this section we will use *Amazon OpenSerach serverless.*\n",
    "\n",
    "Amazon OpenSearch Serverless is a serverless option in Amazon OpenSearch Service. As a developer, you can use OpenSearch Serverless to run petabyte-scale workloads without configuring, managing, and scaling OpenSearch clusters. You get the same interactive millisecond response times as OpenSearch Service with the simplicity of a serverless environment. Pay only for what you use by automatically scaling resources to provide the right amount of capacity for your applicationâ€”without impacting data ingestion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import time\n",
    "vector_store_name = f'bedrock-sample-rag-{suffix}'\n",
    "index_name = f\"bedrock-sample-rag-index-{suffix}\"\n",
    "aoss_client = boto3_session.client('opensearchserverless')\n",
    "bedrock_kb_execution_role = create_bedrock_execution_role(bucket_name=bucket_name)\n",
    "bedrock_kb_execution_role_arn = bedrock_kb_execution_role['Role']['Arn']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an Amazon OpenSeach Serverless collection for the vector store. Note that creation of the collection can take several minutes. You can use the Amazon OpenSearch Serverless console to monitor creation progress.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create security, network and data access policies within OSS\n",
    "encryption_policy, network_policy, access_policy = create_policies_in_oss(vector_store_name=vector_store_name,\n",
    "                       aoss_client=aoss_client,\n",
    "                       bedrock_kb_execution_role_arn=bedrock_kb_execution_role_arn)\n",
    "collection = aoss_client.create_collection(name=vector_store_name,type='VECTORSEARCH')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ 'ResponseMetadata': { 'HTTPHeaders': { 'connection': 'keep-alive',\n",
      "                                         'content-length': '314',\n",
      "                                         'content-type': 'application/x-amz-json-1.0',\n",
      "                                         'date': 'Sun, 26 Jan 2025 19:30:06 '\n",
      "                                                 'GMT',\n",
      "                                         'x-amzn-requestid': 'c8aec0d3-0a3e-43f2-8bec-f16edd78b2e4'},\n",
      "                        'HTTPStatusCode': 200,\n",
      "                        'RequestId': 'c8aec0d3-0a3e-43f2-8bec-f16edd78b2e4',\n",
      "                        'RetryAttempts': 0},\n",
      "  'createCollectionDetail': { 'arn': 'arn:aws:aoss:us-east-1:266735831348:collection/a3bpnvu4pkwa0jxhqlrk',\n",
      "                              'createdDate': 1737919806812,\n",
      "                              'id': 'a3bpnvu4pkwa0jxhqlrk',\n",
      "                              'kmsKeyArn': 'auto',\n",
      "                              'lastModifiedDate': 1737919806812,\n",
      "                              'name': 'bedrock-sample-rag-203',\n",
      "                              'standbyReplicas': 'ENABLED',\n",
      "                              'status': 'CREATING',\n",
      "                              'type': 'VECTORSEARCH'}}\n"
     ]
    }
   ],
   "source": [
    "pp.pprint(collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'encryption_policy' (dict)\n",
      "Stored 'network_policy' (dict)\n",
      "Stored 'access_policy' (dict)\n",
      "Stored 'collection' (dict)\n"
     ]
    }
   ],
   "source": [
    "%store encryption_policy network_policy access_policy collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a3bpnvu4pkwa0jxhqlrk.us-east-1.aoss.amazonaws.com\n"
     ]
    }
   ],
   "source": [
    "# Get the OpenSearch serverless collection URL\n",
    "collection_id = collection['createCollectionDetail']['id']\n",
    "host = collection_id + '.' + region_name + '.aoss.amazonaws.com'\n",
    "print(host)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating collection...\n",
      "..............................\n",
      "Collection successfully created:\n",
      "[ { 'arn': 'arn:aws:aoss:us-east-1:266735831348:collection/a3bpnvu4pkwa0jxhqlrk',\n",
      "    'collectionEndpoint': 'https://a3bpnvu4pkwa0jxhqlrk.us-east-1.aoss.amazonaws.com',\n",
      "    'createdDate': 1737919806812,\n",
      "    'dashboardEndpoint': 'https://a3bpnvu4pkwa0jxhqlrk.us-east-1.aoss.amazonaws.com/_dashboards',\n",
      "    'id': 'a3bpnvu4pkwa0jxhqlrk',\n",
      "    'kmsKeyArn': 'auto',\n",
      "    'lastModifiedDate': 1737919829924,\n",
      "    'name': 'bedrock-sample-rag-203',\n",
      "    'standbyReplicas': 'ENABLED',\n",
      "    'status': 'ACTIVE',\n",
      "    'type': 'VECTORSEARCH'}]\n"
     ]
    }
   ],
   "source": [
    "# wait for collection creation\n",
    "# This can take couple of minutes to finish\n",
    "response = aoss_client.batch_get_collection(names=[vector_store_name])\n",
    "# Periodically check collection status\n",
    "while (response['collectionDetails'][0]['status']) == 'CREATING':\n",
    "    print('Creating collection...')\n",
    "    interactive_sleep(30)\n",
    "    response = aoss_client.batch_get_collection(names=[vector_store_name])\n",
    "print('\\nCollection successfully created:')\n",
    "pp.pprint(response[\"collectionDetails\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opensearch serverless arn:  arn:aws:iam::266735831348:policy/AmazonBedrockOSSPolicyForKnowledgeBase_428\n",
      "............................................................\r"
     ]
    }
   ],
   "source": [
    "# create opensearch serverless access policy and attach it to Bedrock execution role\n",
    "try:\n",
    "    create_oss_policy_attach_bedrock_execution_role(collection_id=collection_id,\n",
    "                                                    bedrock_kb_execution_role=bedrock_kb_execution_role)\n",
    "    # It can take up to a minute for data access rules to be enforced\n",
    "    interactive_sleep(60)\n",
    "except Exception as e:\n",
    "    print(\"Policy already exists\")\n",
    "    pp.pprint(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 - Create vector index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the vector index in Opensearch serverless, with the knn_vector field index mapping, specifying the dimension size, name and engine.\n",
    "from opensearchpy import OpenSearch, RequestsHttpConnection, AWSV4SignerAuth, RequestError\n",
    "credentials = boto3.Session().get_credentials()\n",
    "awsauth = auth = AWSV4SignerAuth(credentials, region_name, service)\n",
    "\n",
    "index_name = f\"bedrock-sample-index-{suffix}\"\n",
    "body_json = {\n",
    "   \"settings\": {\n",
    "      \"index.knn\": \"true\",\n",
    "       \"number_of_shards\": 1,\n",
    "       \"knn.algo_param.ef_search\": 512,\n",
    "       \"number_of_replicas\": 0,\n",
    "   },\n",
    "   \"mappings\": {\n",
    "      \"properties\": {\n",
    "         \"vector\": {\n",
    "            \"type\": \"knn_vector\",\n",
    "            \"dimension\": 1536,\n",
    "             \"method\": {\n",
    "                 \"name\": \"hnsw\",\n",
    "                 \"engine\": \"faiss\",\n",
    "                 \"space_type\": \"l2\"\n",
    "             },\n",
    "         },\n",
    "         \"text\": {\n",
    "            \"type\": \"text\"\n",
    "         },\n",
    "         \"text-metadata\": {\n",
    "            \"type\": \"text\"         }\n",
    "      }\n",
    "   }\n",
    "}\n",
    "\n",
    "# Build the OpenSearch client\n",
    "oss_client = OpenSearch(\n",
    "    hosts=[{'host': host, 'port': 443}],\n",
    "    http_auth=awsauth,\n",
    "    use_ssl=True,\n",
    "    verify_certs=True,\n",
    "    connection_class=RequestsHttpConnection,\n",
    "    timeout=300\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating index:\n",
      "{ 'acknowledged': True,\n",
      "  'index': 'bedrock-sample-index-203',\n",
      "  'shards_acknowledged': True}\n",
      "............................................................\r"
     ]
    }
   ],
   "source": [
    "# Create index\n",
    "try:\n",
    "    response = oss_client.indices.create(index=index_name, body=json.dumps(body_json))\n",
    "    print('\\nCreating index:')\n",
    "    pp.pprint(response)\n",
    "\n",
    "    # index creation can take up to a minute\n",
    "    interactive_sleep(60)\n",
    "except RequestError as e:\n",
    "    # you can delete the index if its already exists\n",
    "    # oss_client.indices.delete(index=index_name)\n",
    "    print(f'Error while trying to create the index, with error {e.error}\\nyou may unmark the delete above to delete, and recreate the index')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download data to ingest into our knowledge base\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and prepare dataset\n",
    "#!mkdir -p ./data\n",
    "\n",
    "from urllib.request import urlretrieve\n",
    "urls = [\n",
    "    'https://s2.q4cdn.com/299287126/files/doc_financials/2023/ar/2022-Shareholder-Letter.pdf',\n",
    "    'https://s2.q4cdn.com/299287126/files/doc_financials/2022/ar/2021-Shareholder-Letter.pdf',\n",
    "    'https://s2.q4cdn.com/299287126/files/doc_financials/2021/ar/Amazon-2020-Shareholder-Letter-and-1997-Shareholder-Letter.pdf',\n",
    "    'https://s2.q4cdn.com/299287126/files/doc_financials/2020/ar/2019-Shareholder-Letter.pdf'\n",
    "]\n",
    "\n",
    "filenames = [\n",
    "    'AMZN-2022-Shareholder-Letter.pdf',\n",
    "    'AMZN-2021-Shareholder-Letter.pdf',\n",
    "    'AMZN-2020-Shareholder-Letter.pdf',\n",
    "    'AMZN-2019-Shareholder-Letter.pdf'\n",
    "]\n",
    "\n",
    "data_root = \"data/\"\n",
    "\n",
    "for idx, url in enumerate(urls):\n",
    "    file_path = data_root + filenames[idx]\n",
    "    urlretrieve(url, file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Upload data to S3 Bucket data source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload data to s3 to the bucket that was configured as a data source to the knowledge base\n",
    "s3_client = boto3.client(\"s3\")\n",
    "def uploadDirectory(path,bucket_name):\n",
    "        for root,dirs,files in os.walk(path):\n",
    "            for file in files:\n",
    "                s3_client.upload_file(os.path.join(root,file),bucket_name,file)\n",
    "\n",
    "uploadDirectory(data_root, bucket_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Knowledge Base\n",
    "Steps:\n",
    "- initialize Open search serverless configuration which will include collection ARN, index name, vector field, text field and metadata field.\n",
    "- initialize chunking strategy, based on which KB will split the documents into pieces of size equal to the chunk size mentioned in the `chunkingStrategyConfiguration`.\n",
    "- initialize the s3 configuration, which will be used to create the data source object later.\n",
    "- initialize the Titan embeddings model ARN, as this will be used to create the embeddings for each of the text chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "opensearchServerlessConfiguration = {\n",
    "            \"collectionArn\": collection[\"createCollectionDetail\"]['arn'],\n",
    "            \"vectorIndexName\": index_name,\n",
    "            \"fieldMapping\": {\n",
    "                \"vectorField\": \"vector\",\n",
    "                \"textField\": \"text\",\n",
    "                \"metadataField\": \"text-metadata\"\n",
    "            }\n",
    "        }\n",
    "\n",
    "# Ingest strategy - How to ingest data from the data source\n",
    "chunkingStrategyConfiguration = {\n",
    "    \"chunkingStrategy\": \"FIXED_SIZE\",\n",
    "    \"fixedSizeChunkingConfiguration\": {\n",
    "        \"maxTokens\": 512,\n",
    "        \"overlapPercentage\": 20\n",
    "    }\n",
    "}\n",
    "\n",
    "# The data source to ingest documents from, into the OpenSearch serverless knowledge base index\n",
    "s3Configuration = {\n",
    "    \"bucketArn\": f\"arn:aws:s3:::{bucket_name}\",\n",
    "    # \"inclusionPrefixes\":[\"*.*\"] # you can use this if you want to create a KB using data within s3 prefixes.\n",
    "}\n",
    "\n",
    "# The embedding model used by Bedrock to embed ingested documents, and realtime prompts\n",
    "embeddingModelArn = f\"arn:aws:bedrock:{region_name}::foundation-model/amazon.titan-embed-text-v1\"\n",
    "\n",
    "name = f\"bedrock-sample-knowledge-base-{suffix}\"\n",
    "description = \"Amazon shareholder letter knowledge base.\"\n",
    "roleArn = bedrock_kb_execution_role_arn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Provide the above configurations as input to the `create_knowledge_base` method, which will create the Knowledge base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a KnowledgeBase\n",
    "from retrying import retry\n",
    "\n",
    "@retry(wait_random_min=1000, wait_random_max=2000,stop_max_attempt_number=7)\n",
    "def create_knowledge_base_func():\n",
    "    create_kb_response = bedrock_agent_client.create_knowledge_base(\n",
    "        name = name,\n",
    "        description = description,\n",
    "        roleArn = roleArn,\n",
    "        knowledgeBaseConfiguration = {\n",
    "            \"type\": \"VECTOR\",\n",
    "            \"vectorKnowledgeBaseConfiguration\": {\n",
    "                \"embeddingModelArn\": embeddingModelArn\n",
    "            }\n",
    "        },\n",
    "        storageConfiguration = {\n",
    "            \"type\": \"OPENSEARCH_SERVERLESS\",\n",
    "            \"opensearchServerlessConfiguration\":opensearchServerlessConfiguration\n",
    "        }\n",
    "    )\n",
    "    return create_kb_response[\"knowledgeBase\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    kb = create_knowledge_base_func()\n",
    "except Exception as err:\n",
    "    print(f\"{err=}, {type(err)=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ 'createdAt': datetime.datetime(2025, 1, 26, 19, 48, 25, 497322, tzinfo=tzutc()),\n",
      "  'description': 'Amazon shareholder letter knowledge base.',\n",
      "  'knowledgeBaseArn': 'arn:aws:bedrock:us-east-1:266735831348:knowledge-base/UGNQP7Y6FS',\n",
      "  'knowledgeBaseConfiguration': { 'type': 'VECTOR',\n",
      "                                  'vectorKnowledgeBaseConfiguration': { 'embeddingModelArn': 'arn:aws:bedrock:us-east-1::foundation-model/amazon.titan-embed-text-v1'}},\n",
      "  'knowledgeBaseId': 'UGNQP7Y6FS',\n",
      "  'name': 'bedrock-sample-knowledge-base-203',\n",
      "  'roleArn': 'arn:aws:iam::266735831348:role/AmazonBedrockExecutionRoleForKnowledgeBase_428',\n",
      "  'status': 'CREATING',\n",
      "  'storageConfiguration': { 'opensearchServerlessConfiguration': { 'collectionArn': 'arn:aws:aoss:us-east-1:266735831348:collection/a3bpnvu4pkwa0jxhqlrk',\n",
      "                                                                   'fieldMapping': { 'metadataField': 'text-metadata',\n",
      "                                                                                     'textField': 'text',\n",
      "                                                                                     'vectorField': 'vector'},\n",
      "                                                                   'vectorIndexName': 'bedrock-sample-index-203'},\n",
      "                            'type': 'OPENSEARCH_SERVERLESS'},\n",
      "  'updatedAt': datetime.datetime(2025, 1, 26, 19, 48, 25, 497322, tzinfo=tzutc())}\n"
     ]
    }
   ],
   "source": [
    "pp.pprint(kb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get KnowledgeBase \n",
    "get_kb_response = bedrock_agent_client.get_knowledge_base(knowledgeBaseId = kb['knowledgeBaseId'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to create a data source, which will be associated with the knowledge base created above. Once the data source is ready, we can then start to ingest the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ 'createdAt': datetime.datetime(2025, 1, 26, 19, 57, 6, 8872, tzinfo=tzutc()),\n",
      "  'dataDeletionPolicy': 'DELETE',\n",
      "  'dataSourceConfiguration': { 's3Configuration': { 'bucketArn': 'arn:aws:s3:::bedrock-kb-us-east-1-266735831348'},\n",
      "                               'type': 'S3'},\n",
      "  'dataSourceId': 'RGUI1UMIW2',\n",
      "  'description': 'Amazon shareholder letter knowledge base.',\n",
      "  'knowledgeBaseId': 'UGNQP7Y6FS',\n",
      "  'name': 'bedrock-sample-knowledge-base-203',\n",
      "  'status': 'AVAILABLE',\n",
      "  'updatedAt': datetime.datetime(2025, 1, 26, 19, 57, 6, 8872, tzinfo=tzutc()),\n",
      "  'vectorIngestionConfiguration': { 'chunkingConfiguration': { 'chunkingStrategy': 'FIXED_SIZE',\n",
      "                                                               'fixedSizeChunkingConfiguration': { 'maxTokens': 512,\n",
      "                                                                                                   'overlapPercentage': 20}}}}\n"
     ]
    }
   ],
   "source": [
    "# Create a DataSource in KnowledgeBase \n",
    "create_ds_response = bedrock_agent_client.create_data_source(\n",
    "    name = name,\n",
    "    description = description,\n",
    "    knowledgeBaseId = kb['knowledgeBaseId'],\n",
    "    dataSourceConfiguration = {\n",
    "        \"type\": \"S3\",\n",
    "        \"s3Configuration\":s3Configuration\n",
    "    },\n",
    "    vectorIngestionConfiguration = {\n",
    "        \"chunkingConfiguration\": chunkingStrategyConfiguration\n",
    "    }\n",
    ")\n",
    "ds = create_ds_response[\"dataSource\"]\n",
    "pp.pprint(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': 'f8eb02cc-5d95-4ace-baa5-927a07842fd1',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'date': 'Sun, 26 Jan 2025 19:57:24 GMT',\n",
       "   'content-type': 'application/json',\n",
       "   'content-length': '603',\n",
       "   'connection': 'keep-alive',\n",
       "   'x-amzn-requestid': 'f8eb02cc-5d95-4ace-baa5-927a07842fd1',\n",
       "   'x-amz-apigw-id': 'FAwBzHXOoAMEbFg=',\n",
       "   'x-amzn-trace-id': 'Root=1-679693a4-6ec4ceec14e4ee506392adbe'},\n",
       "  'RetryAttempts': 0},\n",
       " 'dataSource': {'createdAt': datetime.datetime(2025, 1, 26, 19, 57, 6, 8872, tzinfo=tzutc()),\n",
       "  'dataDeletionPolicy': 'DELETE',\n",
       "  'dataSourceConfiguration': {'s3Configuration': {'bucketArn': 'arn:aws:s3:::bedrock-kb-us-east-1-266735831348'},\n",
       "   'type': 'S3'},\n",
       "  'dataSourceId': 'RGUI1UMIW2',\n",
       "  'description': 'Amazon shareholder letter knowledge base.',\n",
       "  'knowledgeBaseId': 'UGNQP7Y6FS',\n",
       "  'name': 'bedrock-sample-knowledge-base-203',\n",
       "  'status': 'AVAILABLE',\n",
       "  'updatedAt': datetime.datetime(2025, 1, 26, 19, 57, 6, 8872, tzinfo=tzutc()),\n",
       "  'vectorIngestionConfiguration': {'chunkingConfiguration': {'chunkingStrategy': 'FIXED_SIZE',\n",
       "    'fixedSizeChunkingConfiguration': {'maxTokens': 512,\n",
       "     'overlapPercentage': 20}}}}}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get DataSource \n",
    "bedrock_agent_client.get_data_source(knowledgeBaseId = kb['knowledgeBaseId'], dataSourceId = ds[\"dataSourceId\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start ingestion job\n",
    "Once the KB and data source is created, we can start the ingestion job.\n",
    "During the ingestion job, KB will fetch the documents in the data source, pre-process it to extract text, chunk it based on the chunking size provided, create embeddings of each chunk and then write it to the vector database, in this case OSS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..............................\r"
     ]
    }
   ],
   "source": [
    "# Start an ingestion job\n",
    "interactive_sleep(30)\n",
    "start_job_response = bedrock_agent_client.start_ingestion_job(knowledgeBaseId = kb['knowledgeBaseId'], dataSourceId = ds[\"dataSourceId\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ 'dataSourceId': 'RGUI1UMIW2',\n",
      "  'ingestionJobId': 'UCKBALTTYP',\n",
      "  'knowledgeBaseId': 'UGNQP7Y6FS',\n",
      "  'startedAt': datetime.datetime(2025, 1, 26, 19, 58, 37, 97693, tzinfo=tzutc()),\n",
      "  'statistics': { 'numberOfDocumentsDeleted': 0,\n",
      "                  'numberOfDocumentsFailed': 0,\n",
      "                  'numberOfDocumentsScanned': 0,\n",
      "                  'numberOfMetadataDocumentsModified': 0,\n",
      "                  'numberOfMetadataDocumentsScanned': 0,\n",
      "                  'numberOfModifiedDocumentsIndexed': 0,\n",
      "                  'numberOfNewDocumentsIndexed': 0},\n",
      "  'status': 'STARTING',\n",
      "  'updatedAt': datetime.datetime(2025, 1, 26, 19, 58, 37, 97693, tzinfo=tzutc())}\n"
     ]
    }
   ],
   "source": [
    "job = start_job_response[\"ingestionJob\"]\n",
    "pp.pprint(job)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ 'dataSourceId': 'RGUI1UMIW2',\n",
      "  'ingestionJobId': 'UCKBALTTYP',\n",
      "  'knowledgeBaseId': 'UGNQP7Y6FS',\n",
      "  'startedAt': datetime.datetime(2025, 1, 26, 19, 58, 37, 97693, tzinfo=tzutc()),\n",
      "  'statistics': { 'numberOfDocumentsDeleted': 0,\n",
      "                  'numberOfDocumentsFailed': 0,\n",
      "                  'numberOfDocumentsScanned': 8,\n",
      "                  'numberOfMetadataDocumentsModified': 0,\n",
      "                  'numberOfMetadataDocumentsScanned': 0,\n",
      "                  'numberOfModifiedDocumentsIndexed': 0,\n",
      "                  'numberOfNewDocumentsIndexed': 8},\n",
      "  'status': 'COMPLETE',\n",
      "  'updatedAt': datetime.datetime(2025, 1, 26, 19, 58, 53, 528890, tzinfo=tzutc())}\n"
     ]
    }
   ],
   "source": [
    "# Get job \n",
    "while(job['status']!='COMPLETE' ):\n",
    "    get_job_response = bedrock_agent_client.get_ingestion_job(\n",
    "      knowledgeBaseId = kb['knowledgeBaseId'],\n",
    "        dataSourceId = ds[\"dataSourceId\"],\n",
    "        ingestionJobId = job[\"ingestionJobId\"]\n",
    "  )\n",
    "    job = get_job_response[\"ingestionJob\"]\n",
    "    \n",
    "    interactive_sleep(30)\n",
    "\n",
    "pp.pprint(job)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'UGNQP7Y6FS'\n"
     ]
    }
   ],
   "source": [
    "# Print the knowledge base Id in bedrock, that corresponds to the Opensearch index in the collection we created before, we will use it for the invocation later\n",
    "kb_id = kb[\"knowledgeBaseId\"]\n",
    "pp.pprint(kb_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'kb_id' (str)\n"
     ]
    }
   ],
   "source": [
    "# keep the kb_id for invocation later in the invoke request\n",
    "%store kb_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the knowledge base\n",
    "### Note: If you plan to run any following notebooks, you can skip this section\n",
    "### Using RetrieveAndGenerate API\n",
    "Behind the scenes, RetrieveAndGenerate API converts queries into embeddings, searches the knowledge base, and then augments the foundation model prompt with the search results as context information and returns the FM-generated response to the question. For multi-turn conversations, Knowledge Bases manage short-term memory of the conversation to provide more contextual results.\n",
    "\n",
    "The output of the RetrieveAndGenerate API includes the generated response, source attribution as well as the retrieved text chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try out KB using RetrieveAndGenerate API\n",
    "bedrock_agent_runtime_client = boto3.client(\"bedrock-agent-runtime\", region_name=region_name)\n",
    "# Lets see how different Anthropic Claude 3 models responds to the input text we provide\n",
    "claude_model_ids = [ [\"Claude 3 Sonnet\", \"anthropic.claude-3-sonnet-20240229-v1:0\"], [\"Claude 3 Haiku\", \"anthropic.claude-3-haiku-20240307-v1:0\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_bedrock_llm_with_knowledge_base(query: str, model_arn: str, kb_id: str) -> str:\n",
    "    response = bedrock_agent_runtime_client.retrieve_and_generate(\n",
    "        input={\n",
    "            'text': query\n",
    "        },\n",
    "        retrieveAndGenerateConfiguration={\n",
    "            'type': 'KNOWLEDGE_BASE',\n",
    "            'knowledgeBaseConfiguration': {\n",
    "                'knowledgeBaseId': kb_id,\n",
    "                'modelArn': model_arn\n",
    "            }\n",
    "        },\n",
    "    )\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- Generated using Claude 3 Sonnet:\n",
      "('Amazon has been working on large language models (LLMs) for generative AI '\n",
      " 'for some time. The company believes generative AI will transform and improve '\n",
      " 'virtually every customer experience, and plans to invest substantially in '\n",
      " 'these models across its consumer, seller, brand, and creator experiences. '\n",
      " 'AWS is offering machine learning chips like Trainium and Inferentia that '\n",
      " 'provide price-performance benefits, allowing companies of all sizes to '\n",
      " 'afford to train and run their LLMs in production. AWS also enables companies '\n",
      " 'to choose from various LLMs and build applications with AWS security, '\n",
      " \"privacy and other features. One example application is AWS's CodeWhisperer, \"\n",
      " 'which generates code suggestions in real time to boost developer '\n",
      " 'productivity.')\n",
      "---------- The citations for the response generated by Claude 3 Sonnet:\n",
      "[ 'Amazon has been using machine learning extensively for 25 years, employing '\n",
      "  'it in everything from personalized ecommerce recommendations, to '\n",
      "  'fulfillment center pick paths, to drones for Prime Air, to Alexa, to the '\n",
      "  'many machine learning services AWS offers (where AWS has the broadest '\n",
      "  'machine learning functionality and customer base of any cloud provider). '\n",
      "  'More recently, a newer form of machine learning, called Generative AI, has '\n",
      "  'burst onto the scene and promises to significantly accelerate machine '\n",
      "  'learning adoption. Generative AI is based on very Large Language Models '\n",
      "  '(trained on up to hundreds of billions of parameters, and growing), across '\n",
      "  'expansive datasets, and has radically general and broad recall and learning '\n",
      "  'capabilities. We have been working on our own LLMs for a while now, believe '\n",
      "  'it will transform and improve virtually every customer experience, and will '\n",
      "  'continue to invest substantially in these models across all of our '\n",
      "  'consumer, seller, brand, and creator experiences. Additionally, as weâ€™ve '\n",
      "  'done for years in AWS, weâ€™re democratizing this technology so companies of '\n",
      "  'all sizes can leverage Generative AI. AWS is offering the most '\n",
      "  'price-performant machine learning chips in Trainium and Inferentia so small '\n",
      "  'and large companies can afford to train and run their LLMs in production. '\n",
      "  'We enable companies to choose from various LLMs and build applications with '\n",
      "  'all of the AWS security, privacy and other features that customers are '\n",
      "  'accustomed to using. And, weâ€™re delivering applications like AWSâ€™s '\n",
      "  'CodeWhisperer, which revolutionizesdeveloper productivity by generating '\n",
      "  'code suggestions in real time. I could write an entire letter on LLMs and '\n",
      "  'Generative AI as I think they will be that transformative, but Iâ€™ll leave '\n",
      "  'that for a future letter. Letâ€™s just say that LLMs and Generative AI are '\n",
      "  'going to be a big deal for customers, our shareholders, and Amazon.     So, '\n",
      "  'in closing, Iâ€™m optimistic that weâ€™ll emerge from this challenging '\n",
      "  'macroeconomic time in a stronger position than when we entered it. There '\n",
      "  'are several reasons for it and Iâ€™ve mentioned many of them above. But, '\n",
      "  'there are two relatively simple statistics that underline our immense '\n",
      "  'future opportunity. While we have a consumer business thatâ€™s $434B in 2022, '\n",
      "  'the vast majority of total market segment share in global retail still '\n",
      "  'resides in physical stores (roughly 80%). And, itâ€™s a similar story for '\n",
      "  'Global IT spending, where we have AWS revenue of $80B in 2022, with about '\n",
      "  '90% of Global IT spending still on-premises and yet to migrate to the '\n",
      "  'cloud.',\n",
      "  'Amazon has been using machine learning extensively for 25 years, employing '\n",
      "  'it in everything from personalized ecommerce recommendations, to '\n",
      "  'fulfillment center pick paths, to drones for Prime Air, to Alexa, to the '\n",
      "  'many machine learning services AWS offers (where AWS has the broadest '\n",
      "  'machine learning functionality and customer base of any cloud provider). '\n",
      "  'More recently, a newer form of machine learning, called Generative AI, has '\n",
      "  'burst onto the scene and promises to significantly accelerate machine '\n",
      "  'learning adoption. Generative AI is based on very Large Language Models '\n",
      "  '(trained on up to hundreds of billions of parameters, and growing), across '\n",
      "  'expansive datasets, and has radically general and broad recall and learning '\n",
      "  'capabilities. We have been working on our own LLMs for a while now, believe '\n",
      "  'it will transform and improve virtually every customer experience, and will '\n",
      "  'continue to invest substantially in these models across all of our '\n",
      "  'consumer, seller, brand, and creator experiences. Additionally, as weâ€™ve '\n",
      "  'done for years in AWS, weâ€™re democratizing this technology so companies of '\n",
      "  'all sizes can leverage Generative AI. AWS is offering the most '\n",
      "  'price-performant machine learning chips in Trainium and Inferentia so small '\n",
      "  'and large companies can afford to train and run their LLMs in production. '\n",
      "  'We enable companies to choose from various LLMs and build applications with '\n",
      "  'all of the AWS security, privacy and other features that customers are '\n",
      "  'accustomed to using. And, weâ€™re delivering applications like AWSâ€™s '\n",
      "  'CodeWhisperer, which revolutionizesdeveloper productivity by generating '\n",
      "  'code suggestions in real time. I could write an entire letter on LLMs and '\n",
      "  'Generative AI as I think they will be that transformative, but Iâ€™ll leave '\n",
      "  'that for a future letter. Letâ€™s just say that LLMs and Generative AI are '\n",
      "  'going to be a big deal for customers, our shareholders, and Amazon.     So, '\n",
      "  'in closing, Iâ€™m optimistic that weâ€™ll emerge from this challenging '\n",
      "  'macroeconomic time in a stronger position than when we entered it. There '\n",
      "  'are several reasons for it and Iâ€™ve mentioned many of them above. But, '\n",
      "  'there are two relatively simple statistics that underline our immense '\n",
      "  'future opportunity. While we have a consumer business thatâ€™s $434B in 2022, '\n",
      "  'the vast majority of total market segment share in global retail still '\n",
      "  'resides in physical stores (roughly 80%). And, itâ€™s a similar story for '\n",
      "  'Global IT spending, where we have AWS revenue of $80B in 2022, with about '\n",
      "  '90% of Global IT spending still on-premises and yet to migrate to the '\n",
      "  'cloud.']\n",
      "\n",
      "---------- Generated using Claude 3 Haiku:\n",
      "('Amazon has been working on developing its own large language models (LLMs) '\n",
      " 'for generative AI, which it believes will transform and improve virtually '\n",
      " 'every customer experience. Amazon sees generative AI as a significant '\n",
      " 'opportunity that will accelerate machine learning adoption across its '\n",
      " 'consumer, seller, brand, and creator experiences. To democratize generative '\n",
      " 'AI technology, Amazon is offering the most price-performant machine learning '\n",
      " 'chips in Trainium and Inferentia through AWS, enabling companies of all '\n",
      " 'sizes to afford to train and run their LLMs in production. AWS also enables '\n",
      " \"companies to choose from various LLMs and build applications with AWS's \"\n",
      " 'security, privacy, and other features.')\n",
      "---------- The citations for the response generated by Claude 3 Haiku:\n",
      "[ 'Amazon has been using machine learning extensively for 25 years, employing '\n",
      "  'it in everything from personalized ecommerce recommendations, to '\n",
      "  'fulfillment center pick paths, to drones for Prime Air, to Alexa, to the '\n",
      "  'many machine learning services AWS offers (where AWS has the broadest '\n",
      "  'machine learning functionality and customer base of any cloud provider). '\n",
      "  'More recently, a newer form of machine learning, called Generative AI, has '\n",
      "  'burst onto the scene and promises to significantly accelerate machine '\n",
      "  'learning adoption. Generative AI is based on very Large Language Models '\n",
      "  '(trained on up to hundreds of billions of parameters, and growing), across '\n",
      "  'expansive datasets, and has radically general and broad recall and learning '\n",
      "  'capabilities. We have been working on our own LLMs for a while now, believe '\n",
      "  'it will transform and improve virtually every customer experience, and will '\n",
      "  'continue to invest substantially in these models across all of our '\n",
      "  'consumer, seller, brand, and creator experiences. Additionally, as weâ€™ve '\n",
      "  'done for years in AWS, weâ€™re democratizing this technology so companies of '\n",
      "  'all sizes can leverage Generative AI. AWS is offering the most '\n",
      "  'price-performant machine learning chips in Trainium and Inferentia so small '\n",
      "  'and large companies can afford to train and run their LLMs in production. '\n",
      "  'We enable companies to choose from various LLMs and build applications with '\n",
      "  'all of the AWS security, privacy and other features that customers are '\n",
      "  'accustomed to using. And, weâ€™re delivering applications like AWSâ€™s '\n",
      "  'CodeWhisperer, which revolutionizesdeveloper productivity by generating '\n",
      "  'code suggestions in real time. I could write an entire letter on LLMs and '\n",
      "  'Generative AI as I think they will be that transformative, but Iâ€™ll leave '\n",
      "  'that for a future letter. Letâ€™s just say that LLMs and Generative AI are '\n",
      "  'going to be a big deal for customers, our shareholders, and Amazon.     So, '\n",
      "  'in closing, Iâ€™m optimistic that weâ€™ll emerge from this challenging '\n",
      "  'macroeconomic time in a stronger position than when we entered it. There '\n",
      "  'are several reasons for it and Iâ€™ve mentioned many of them above. But, '\n",
      "  'there are two relatively simple statistics that underline our immense '\n",
      "  'future opportunity. While we have a consumer business thatâ€™s $434B in 2022, '\n",
      "  'the vast majority of total market segment share in global retail still '\n",
      "  'resides in physical stores (roughly 80%). And, itâ€™s a similar story for '\n",
      "  'Global IT spending, where we have AWS revenue of $80B in 2022, with about '\n",
      "  '90% of Global IT spending still on-premises and yet to migrate to the '\n",
      "  'cloud.',\n",
      "  'Amazon has been using machine learning extensively for 25 years, employing '\n",
      "  'it in everything from personalized ecommerce recommendations, to '\n",
      "  'fulfillment center pick paths, to drones for Prime Air, to Alexa, to the '\n",
      "  'many machine learning services AWS offers (where AWS has the broadest '\n",
      "  'machine learning functionality and customer base of any cloud provider). '\n",
      "  'More recently, a newer form of machine learning, called Generative AI, has '\n",
      "  'burst onto the scene and promises to significantly accelerate machine '\n",
      "  'learning adoption. Generative AI is based on very Large Language Models '\n",
      "  '(trained on up to hundreds of billions of parameters, and growing), across '\n",
      "  'expansive datasets, and has radically general and broad recall and learning '\n",
      "  'capabilities. We have been working on our own LLMs for a while now, believe '\n",
      "  'it will transform and improve virtually every customer experience, and will '\n",
      "  'continue to invest substantially in these models across all of our '\n",
      "  'consumer, seller, brand, and creator experiences. Additionally, as weâ€™ve '\n",
      "  'done for years in AWS, weâ€™re democratizing this technology so companies of '\n",
      "  'all sizes can leverage Generative AI. AWS is offering the most '\n",
      "  'price-performant machine learning chips in Trainium and Inferentia so small '\n",
      "  'and large companies can afford to train and run their LLMs in production. '\n",
      "  'We enable companies to choose from various LLMs and build applications with '\n",
      "  'all of the AWS security, privacy and other features that customers are '\n",
      "  'accustomed to using. And, weâ€™re delivering applications like AWSâ€™s '\n",
      "  'CodeWhisperer, which revolutionizesdeveloper productivity by generating '\n",
      "  'code suggestions in real time. I could write an entire letter on LLMs and '\n",
      "  'Generative AI as I think they will be that transformative, but Iâ€™ll leave '\n",
      "  'that for a future letter. Letâ€™s just say that LLMs and Generative AI are '\n",
      "  'going to be a big deal for customers, our shareholders, and Amazon.     So, '\n",
      "  'in closing, Iâ€™m optimistic that weâ€™ll emerge from this challenging '\n",
      "  'macroeconomic time in a stronger position than when we entered it. There '\n",
      "  'are several reasons for it and Iâ€™ve mentioned many of them above. But, '\n",
      "  'there are two relatively simple statistics that underline our immense '\n",
      "  'future opportunity. While we have a consumer business thatâ€™s $434B in 2022, '\n",
      "  'the vast majority of total market segment share in global retail still '\n",
      "  'resides in physical stores (roughly 80%). And, itâ€™s a similar story for '\n",
      "  'Global IT spending, where we have AWS revenue of $80B in 2022, with about '\n",
      "  '90% of Global IT spending still on-premises and yet to migrate to the '\n",
      "  'cloud.',\n",
      "  'Amazon has been using machine learning extensively for 25 years, employing '\n",
      "  'it in everything from personalized ecommerce recommendations, to '\n",
      "  'fulfillment center pick paths, to drones for Prime Air, to Alexa, to the '\n",
      "  'many machine learning services AWS offers (where AWS has the broadest '\n",
      "  'machine learning functionality and customer base of any cloud provider). '\n",
      "  'More recently, a newer form of machine learning, called Generative AI, has '\n",
      "  'burst onto the scene and promises to significantly accelerate machine '\n",
      "  'learning adoption. Generative AI is based on very Large Language Models '\n",
      "  '(trained on up to hundreds of billions of parameters, and growing), across '\n",
      "  'expansive datasets, and has radically general and broad recall and learning '\n",
      "  'capabilities. We have been working on our own LLMs for a while now, believe '\n",
      "  'it will transform and improve virtually every customer experience, and will '\n",
      "  'continue to invest substantially in these models across all of our '\n",
      "  'consumer, seller, brand, and creator experiences. Additionally, as weâ€™ve '\n",
      "  'done for years in AWS, weâ€™re democratizing this technology so companies of '\n",
      "  'all sizes can leverage Generative AI. AWS is offering the most '\n",
      "  'price-performant machine learning chips in Trainium and Inferentia so small '\n",
      "  'and large companies can afford to train and run their LLMs in production. '\n",
      "  'We enable companies to choose from various LLMs and build applications with '\n",
      "  'all of the AWS security, privacy and other features that customers are '\n",
      "  'accustomed to using. And, weâ€™re delivering applications like AWSâ€™s '\n",
      "  'CodeWhisperer, which revolutionizesdeveloper productivity by generating '\n",
      "  'code suggestions in real time. I could write an entire letter on LLMs and '\n",
      "  'Generative AI as I think they will be that transformative, but Iâ€™ll leave '\n",
      "  'that for a future letter. Letâ€™s just say that LLMs and Generative AI are '\n",
      "  'going to be a big deal for customers, our shareholders, and Amazon.     So, '\n",
      "  'in closing, Iâ€™m optimistic that weâ€™ll emerge from this challenging '\n",
      "  'macroeconomic time in a stronger position than when we entered it. There '\n",
      "  'are several reasons for it and Iâ€™ve mentioned many of them above. But, '\n",
      "  'there are two relatively simple statistics that underline our immense '\n",
      "  'future opportunity. While we have a consumer business thatâ€™s $434B in 2022, '\n",
      "  'the vast majority of total market segment share in global retail still '\n",
      "  'resides in physical stores (roughly 80%). And, itâ€™s a similar story for '\n",
      "  'Global IT spending, where we have AWS revenue of $80B in 2022, with about '\n",
      "  '90% of Global IT spending still on-premises and yet to migrate to the '\n",
      "  'cloud.',\n",
      "  'Amazon has been using machine learning extensively for 25 years, employing '\n",
      "  'it in everything from personalized ecommerce recommendations, to '\n",
      "  'fulfillment center pick paths, to drones for Prime Air, to Alexa, to the '\n",
      "  'many machine learning services AWS offers (where AWS has the broadest '\n",
      "  'machine learning functionality and customer base of any cloud provider). '\n",
      "  'More recently, a newer form of machine learning, called Generative AI, has '\n",
      "  'burst onto the scene and promises to significantly accelerate machine '\n",
      "  'learning adoption. Generative AI is based on very Large Language Models '\n",
      "  '(trained on up to hundreds of billions of parameters, and growing), across '\n",
      "  'expansive datasets, and has radically general and broad recall and learning '\n",
      "  'capabilities. We have been working on our own LLMs for a while now, believe '\n",
      "  'it will transform and improve virtually every customer experience, and will '\n",
      "  'continue to invest substantially in these models across all of our '\n",
      "  'consumer, seller, brand, and creator experiences. Additionally, as weâ€™ve '\n",
      "  'done for years in AWS, weâ€™re democratizing this technology so companies of '\n",
      "  'all sizes can leverage Generative AI. AWS is offering the most '\n",
      "  'price-performant machine learning chips in Trainium and Inferentia so small '\n",
      "  'and large companies can afford to train and run their LLMs in production. '\n",
      "  'We enable companies to choose from various LLMs and build applications with '\n",
      "  'all of the AWS security, privacy and other features that customers are '\n",
      "  'accustomed to using. And, weâ€™re delivering applications like AWSâ€™s '\n",
      "  'CodeWhisperer, which revolutionizesdeveloper productivity by generating '\n",
      "  'code suggestions in real time. I could write an entire letter on LLMs and '\n",
      "  'Generative AI as I think they will be that transformative, but Iâ€™ll leave '\n",
      "  'that for a future letter. Letâ€™s just say that LLMs and Generative AI are '\n",
      "  'going to be a big deal for customers, our shareholders, and Amazon.     So, '\n",
      "  'in closing, Iâ€™m optimistic that weâ€™ll emerge from this challenging '\n",
      "  'macroeconomic time in a stronger position than when we entered it. There '\n",
      "  'are several reasons for it and Iâ€™ve mentioned many of them above. But, '\n",
      "  'there are two relatively simple statistics that underline our immense '\n",
      "  'future opportunity. While we have a consumer business thatâ€™s $434B in 2022, '\n",
      "  'the vast majority of total market segment share in global retail still '\n",
      "  'resides in physical stores (roughly 80%). And, itâ€™s a similar story for '\n",
      "  'Global IT spending, where we have AWS revenue of $80B in 2022, with about '\n",
      "  '90% of Global IT spending still on-premises and yet to migrate to the '\n",
      "  'cloud.']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"What is Amazon's doing in the field of generative AI?\"\n",
    "\n",
    "for model_id in claude_model_ids:\n",
    "    model_arn = f'arn:aws:bedrock:{region_name}::foundation-model/{model_id[1]}'\n",
    "    response = ask_bedrock_llm_with_knowledge_base(query, model_arn, kb_id)\n",
    "    generated_text = response['output']['text']\n",
    "    citations = response[\"citations\"]\n",
    "    contexts = []\n",
    "    for citation in citations:\n",
    "        retrievedReferences = citation[\"retrievedReferences\"]\n",
    "        for reference in retrievedReferences:\n",
    "            contexts.append(reference[\"content\"][\"text\"])\n",
    "    print(f\"---------- Generated using {model_id[0]}:\")\n",
    "    pp.pprint(generated_text )\n",
    "    print(f'---------- The citations for the response generated by {model_id[0]}:')\n",
    "    pp.pprint(contexts)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve API\n",
    "Retrieve API converts user queries into embeddings, searches the knowledge base, and returns the relevant results, giving you more control to build custom workï¬‚ows on top of the semantic search results. The output of the Retrieve API includes the the retrieved text chunks, the location type and URI of the source data, as well as the relevance scores of the retrievals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve api for fetching only the relevant context.\n",
    "relevant_documents = bedrock_agent_runtime_client.retrieve(\n",
    "    retrievalQuery= {\n",
    "        'text': query\n",
    "    },\n",
    "    knowledgeBaseId=kb_id,\n",
    "    retrievalConfiguration= {\n",
    "        'vectorSearchConfiguration': {\n",
    "            'numberOfResults': 3 # will fetch top 3 documents which matches closely with the query.\n",
    "        }\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ { 'content': { 'text': 'Amazon has been using machine learning extensively '\n",
      "                         'for 25 years, employing it in everything from '\n",
      "                         'personalized ecommerce recommendations, to '\n",
      "                         'fulfillment center pick paths, to drones for Prime '\n",
      "                         'Air, to Alexa, to the many machine learning services '\n",
      "                         'AWS offers (where AWS has the broadest machine '\n",
      "                         'learning functionality and customer base of any '\n",
      "                         'cloud provider). More recently, a newer form of '\n",
      "                         'machine learning, called Generative AI, has burst '\n",
      "                         'onto the scene and promises to significantly '\n",
      "                         'accelerate machine learning adoption. Generative AI '\n",
      "                         'is based on very Large Language Models (trained on '\n",
      "                         'up to hundreds of billions of parameters, and '\n",
      "                         'growing), across expansive datasets, and has '\n",
      "                         'radically general and broad recall and learning '\n",
      "                         'capabilities. We have been working on our own LLMs '\n",
      "                         'for a while now, believe it will transform and '\n",
      "                         'improve virtually every customer experience, and '\n",
      "                         'will continue to invest substantially in these '\n",
      "                         'models across all of our consumer, seller, brand, '\n",
      "                         'and creator experiences. Additionally, as weâ€™ve done '\n",
      "                         'for years in AWS, weâ€™re democratizing this '\n",
      "                         'technology so companies of all sizes can leverage '\n",
      "                         'Generative AI. AWS is offering the most '\n",
      "                         'price-performant machine learning chips in Trainium '\n",
      "                         'and Inferentia so small and large companies can '\n",
      "                         'afford to train and run their LLMs in production. We '\n",
      "                         'enable companies to choose from various LLMs and '\n",
      "                         'build applications with all of the AWS security, '\n",
      "                         'privacy and other features that customers are '\n",
      "                         'accustomed to using. And, weâ€™re delivering '\n",
      "                         'applications like AWSâ€™s CodeWhisperer, which '\n",
      "                         'revolutionizesdeveloper productivity by generating '\n",
      "                         'code suggestions in real time. I could write an '\n",
      "                         'entire letter on LLMs and Generative AI as I think '\n",
      "                         'they will be that transformative, but Iâ€™ll leave '\n",
      "                         'that for a future letter. Letâ€™s just say that LLMs '\n",
      "                         'and Generative AI are going to be a big deal for '\n",
      "                         'customers, our shareholders, and Amazon.     So, in '\n",
      "                         'closing, Iâ€™m optimistic that weâ€™ll emerge from this '\n",
      "                         'challenging macroeconomic time in a stronger '\n",
      "                         'position than when we entered it. There are several '\n",
      "                         'reasons for it and Iâ€™ve mentioned many of them '\n",
      "                         'above. But, there are two relatively simple '\n",
      "                         'statistics that underline our immense future '\n",
      "                         'opportunity. While we have a consumer business '\n",
      "                         'thatâ€™s $434B in 2022, the vast majority of total '\n",
      "                         'market segment share in global retail still resides '\n",
      "                         'in physical stores (roughly 80%). And, itâ€™s a '\n",
      "                         'similar story for Global IT spending, where we have '\n",
      "                         'AWS revenue of $80B in 2022, with about 90% of '\n",
      "                         'Global IT spending still on-premises and yet to '\n",
      "                         'migrate to the cloud.',\n",
      "                 'type': 'TEXT'},\n",
      "    'location': { 's3Location': { 'uri': 's3://bedrock-kb-us-east-1-266735831348/AMZN-2022-Shareholder-Letter.pdf'},\n",
      "                  'type': 'S3'},\n",
      "    'metadata': { 'x-amz-bedrock-kb-chunk-id': '1%3A0%3AR7QypJQB9HkGxlKNB9wN',\n",
      "                  'x-amz-bedrock-kb-data-source-id': 'RGUI1UMIW2',\n",
      "                  'x-amz-bedrock-kb-document-page-number': 6.0,\n",
      "                  'x-amz-bedrock-kb-source-uri': 's3://bedrock-kb-us-east-1-266735831348/AMZN-2022-Shareholder-Letter.pdf'},\n",
      "    'score': 0.62741536},\n",
      "  { 'content': { 'text': 'Amazon has been using machine learning extensively '\n",
      "                         'for 25 years, employing it in everything from '\n",
      "                         'personalized ecommerce recommendations, to '\n",
      "                         'fulfillment center pick paths, to drones for Prime '\n",
      "                         'Air, to Alexa, to the many machine learning services '\n",
      "                         'AWS offers (where AWS has the broadest machine '\n",
      "                         'learning functionality and customer base of any '\n",
      "                         'cloud provider). More recently, a newer form of '\n",
      "                         'machine learning, called Generative AI, has burst '\n",
      "                         'onto the scene and promises to significantly '\n",
      "                         'accelerate machine learning adoption. Generative AI '\n",
      "                         'is based on very Large Language Models (trained on '\n",
      "                         'up to hundreds of billions of parameters, and '\n",
      "                         'growing), across expansive datasets, and has '\n",
      "                         'radically general and broad recall and learning '\n",
      "                         'capabilities. We have been working on our own LLMs '\n",
      "                         'for a while now, believe it will transform and '\n",
      "                         'improve virtually every customer experience, and '\n",
      "                         'will continue to invest substantially in these '\n",
      "                         'models across all of our consumer, seller, brand, '\n",
      "                         'and creator experiences. Additionally, as weâ€™ve done '\n",
      "                         'for years in AWS, weâ€™re democratizing this '\n",
      "                         'technology so companies of all sizes can leverage '\n",
      "                         'Generative AI. AWS is offering the most '\n",
      "                         'price-performant machine learning chips in Trainium '\n",
      "                         'and Inferentia so small and large companies can '\n",
      "                         'afford to train and run their LLMs in production. We '\n",
      "                         'enable companies to choose from various LLMs and '\n",
      "                         'build applications with all of the AWS security, '\n",
      "                         'privacy and other features that customers are '\n",
      "                         'accustomed to using. And, weâ€™re delivering '\n",
      "                         'applications like AWSâ€™s CodeWhisperer, which '\n",
      "                         'revolutionizesdeveloper productivity by generating '\n",
      "                         'code suggestions in real time. I could write an '\n",
      "                         'entire letter on LLMs and Generative AI as I think '\n",
      "                         'they will be that transformative, but Iâ€™ll leave '\n",
      "                         'that for a future letter. Letâ€™s just say that LLMs '\n",
      "                         'and Generative AI are going to be a big deal for '\n",
      "                         'customers, our shareholders, and Amazon.     So, in '\n",
      "                         'closing, Iâ€™m optimistic that weâ€™ll emerge from this '\n",
      "                         'challenging macroeconomic time in a stronger '\n",
      "                         'position than when we entered it. There are several '\n",
      "                         'reasons for it and Iâ€™ve mentioned many of them '\n",
      "                         'above. But, there are two relatively simple '\n",
      "                         'statistics that underline our immense future '\n",
      "                         'opportunity. While we have a consumer business '\n",
      "                         'thatâ€™s $434B in 2022, the vast majority of total '\n",
      "                         'market segment share in global retail still resides '\n",
      "                         'in physical stores (roughly 80%). And, itâ€™s a '\n",
      "                         'similar story for Global IT spending, where we have '\n",
      "                         'AWS revenue of $80B in 2022, with about 90% of '\n",
      "                         'Global IT spending still on-premises and yet to '\n",
      "                         'migrate to the cloud.',\n",
      "                 'type': 'TEXT'},\n",
      "    'location': { 's3Location': { 'uri': 's3://bedrock-kb-us-east-1-266735831348/2022-Shareholder-Letter.pdf'},\n",
      "                  'type': 'S3'},\n",
      "    'metadata': { 'x-amz-bedrock-kb-chunk-id': '1%3A0%3AO8cxpJQBbGfQ_D-t_05z',\n",
      "                  'x-amz-bedrock-kb-data-source-id': 'RGUI1UMIW2',\n",
      "                  'x-amz-bedrock-kb-document-page-number': 6.0,\n",
      "                  'x-amz-bedrock-kb-source-uri': 's3://bedrock-kb-us-east-1-266735831348/2022-Shareholder-Letter.pdf'},\n",
      "    'score': 0.62741536},\n",
      "  { 'content': { 'text': 'We believe that weâ€™ve only scratched the surface of '\n",
      "                         'whatâ€™s possible to date, and plan to keep building '\n",
      "                         'the features our business customers tell us they '\n",
      "                         'need and want.     While many brands and merchants '\n",
      "                         'successfully sell their products on Amazonâ€™s '\n",
      "                         'marketplace, there are also a large number of brands '\n",
      "                         'and sellers who have launched their own '\n",
      "                         'direct-to-consumer websites. One of the challenges '\n",
      "                         'for these merchants is driving conversion from views '\n",
      "                         'to purchases. We invented Buy with Prime to help '\n",
      "                         'with this challenge. Buy with Prime allows '\n",
      "                         'third-party brands and sellers to offer their '\n",
      "                         'products on their own websites to our large Amazon '\n",
      "                         'Prime membership, and offer those customers fast, '\n",
      "                         'free Prime shipping and seamless checkout with their '\n",
      "                         'Amazon account. Buy with Prime provides merchants '\n",
      "                         'several additional benefits, including Amazon '\n",
      "                         'handling the product storage, picking, packing, '\n",
      "                         'delivery, payment, and any returns, all through '\n",
      "                         'Amazon Pay and Fulfillment by Amazon. Buy with Prime '\n",
      "                         'has recently been made available to all US '\n",
      "                         'merchants; and so far, Buy with Prime has increased '\n",
      "                         'shopper conversion on third-party shopping sites by '\n",
      "                         '25% on average. Merchants are excited about '\n",
      "                         'converting more sales and fulfilling these shipments '\n",
      "                         'more easily, Prime members love that they can use '\n",
      "                         'their Prime benefits on more destinations, and Buy '\n",
      "                         'with Prime allows us to improve the shopping '\n",
      "                         'experience across more of the web.     Expanding '\n",
      "                         'internationally, pursuing large retail market '\n",
      "                         'segments that are still nascent for Amazon, and '\n",
      "                         'using our unique assets to help merchants sell more '\n",
      "                         'effectively on their own websites are somewhat '\n",
      "                         'natural extensions for us. There are also a few '\n",
      "                         'investments weâ€™re making that are further from our '\n",
      "                         'core businesses, but where we see unique '\n",
      "                         'opportunity. In 2003, AWS would have been a classic '\n",
      "                         'example. In 2023, Amazon Healthcare and Kuiper are '\n",
      "                         'potential analogues.     Our initial efforts in '\n",
      "                         'Healthcare began with pharmacy, which felt less like '\n",
      "                         'a major departure from ecommerce. For years, Amazon '\n",
      "                         'customers had asked us when weâ€™d offer them an '\n",
      "                         'online pharmacy as their frustrations mounted with '\n",
      "                         'current providers. Launched in 2020, Amazon Pharmacy '\n",
      "                         'is a full-service, online pharmacy that offers '\n",
      "                         'transparent pricing, easy refills, and savings for '\n",
      "                         'Prime members. The business is growing quickly, and '\n",
      "                         'continues to innovate. An example is Amazon '\n",
      "                         'Pharmacyâ€™s recent launch of RxPass, which for a $5 '\n",
      "                         'permonth flat fee, enables Prime members to get as '\n",
      "                         'many of the eligible prescription medications as '\n",
      "                         'they need for dozens of common conditions, like high '\n",
      "                         'blood pressure, acid reflux, and anxiety.',\n",
      "                 'type': 'TEXT'},\n",
      "    'location': { 's3Location': { 'uri': 's3://bedrock-kb-us-east-1-266735831348/2022-Shareholder-Letter.pdf'},\n",
      "                  'type': 'S3'},\n",
      "    'metadata': { 'x-amz-bedrock-kb-chunk-id': '1%3A0%3AOMcxpJQBbGfQ_D-t_05z',\n",
      "                  'x-amz-bedrock-kb-data-source-id': 'RGUI1UMIW2',\n",
      "                  'x-amz-bedrock-kb-document-page-number': 5.0,\n",
      "                  'x-amz-bedrock-kb-source-uri': 's3://bedrock-kb-us-east-1-266735831348/2022-Shareholder-Letter.pdf'},\n",
      "    'score': 0.5384084}]\n"
     ]
    }
   ],
   "source": [
    "pp.pprint(relevant_documents[\"retrievalResults\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "br",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
