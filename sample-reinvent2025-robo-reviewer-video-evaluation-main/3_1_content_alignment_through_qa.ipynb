{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<small>Copyright 2025 Amazon.com, Inc. or its affiliates. All Rights Reserved.<br>\n",
    "This is AWS Content subject to the terms of the Customer Agreement</small>\n",
    "\n",
    "# Module 3.1: Content Alignment Evaluation Through Q&A\n",
    "\n",
    "This notebook demonstrates the video evaluation pipeline using Amazon Nova Premier to assess video-text alignment across multiple focus areas. We'll explore how to generate structured Q&A pairs and evaluate video content against text prompts.\n",
    "\n",
    "## A. Overview\n",
    "\n",
    "The video evaluation pipeline consists of:\n",
    "1. **Q&A Generation** - Create structured questions based on video prompts\n",
    "2. **Video Analysis** - Use multimodal AI to answer questions about videos\n",
    "3. **Alignment Scoring** - Calculate alignment scores across focus areas\n",
    "4. **Complete Pipeline** - End-to-end evaluation with S3 storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Install Dependencies\n",
    "\n",
    "First, let's install the required packages for this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q matplotlib opencv-python Pillow tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import time\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "from utils.content_alignment import generate_qa_alignment, evaluate_video_qa, evaluation_pipeline\n",
    "from utils.config import get_s3_bucket, discover_video_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## D. Configuration\n",
    "\n",
    "Configure your AWS session and evaluation parameters. Make sure your video files and corresponding prompt files exist in S3.\n",
    "\n",
    "- S3 bucket and video files will be automatically detected\n",
    "- The notebook will use the first available video with a corresponding prompt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AWS Configuration\n",
    "session = boto3.Session()\n",
    "\n",
    "# Get S3 bucket name\n",
    "S3_BUCKET = get_s3_bucket(session)\n",
    "\n",
    "# Load configuration for video prefix\n",
    "with open('config.json', 'r') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "# Discover available videos\n",
    "available_videos = discover_video_files(\n",
    "    session, \n",
    "    S3_BUCKET, \n",
    "    config['video_prefix']\n",
    ")\n",
    "\n",
    "if available_videos:\n",
    "    print(f\"üìπ Found {len(available_videos)} video(s) with prompts:\")\n",
    "    for i, video in enumerate(available_videos, 1):\n",
    "        print(f\"   {i}. {video}\")\n",
    "    \n",
    "    # Use the first available video\n",
    "    VIDEO_NAME = available_videos[0]\n",
    "    print(f\"\\nüéØ Using video: {VIDEO_NAME}\")\n",
    "else:\n",
    "    print(\"‚ùå No videos with prompts found. Please generate videos first.\")\n",
    "    VIDEO_NAME = \"example.mp4\"  # Fallback\n",
    "\n",
    "S3_VIDEO_URI = f\"s3://{S3_BUCKET}/{config['video_prefix']}{VIDEO_NAME}\"\n",
    "MODEL_ID = \"us.amazon.nova-premier-v1:0\"\n",
    "\n",
    "# Focus areas for evaluation\n",
    "FOCUS_AREAS = [\n",
    "    \"subject_alignment\",\n",
    "    \"background_alignment\", \n",
    "    \"color_accuracy\",\n",
    "    \"activity_alignment\",\n",
    "    \"spatial_relationships\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## E. Step 1 - Generate Q&A Pairs\n",
    "\n",
    "The first step creates structured questions and answers based on the video prompt and a specific focus area. The `generate_qa_alignment` function:\n",
    "\n",
    "**Key Operations:**\n",
    "1. **Atomic decomposition** - Breaks down video descriptions into smallest meaningful units\n",
    "2. **Focus filtering** - Keeps only tuples relevant to the specified focus area\n",
    "3. **Question generation** - Creates 5 targeted questions with multiple choice answers\n",
    "4. **Answer positioning** - Provides correct answers for evaluation scoring\n",
    "\n",
    "**Why this matters:** Structured Q&A pairs enable objective, measurable evaluation of video-text alignment across specific aspects like subjects, backgrounds, colors, activities, and spatial relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read video prompt from S3\n",
    "s3_client = session.client('s3')\n",
    "\n",
    "# Extract prompt file path from video URI\n",
    "prompt_uri = S3_VIDEO_URI.replace('.mp4', '_prompt.txt')\n",
    "bucket = S3_VIDEO_URI.split('/')[2]\n",
    "prompt_key = '/'.join(prompt_uri.split('/')[3:])\n",
    "\n",
    "try:\n",
    "    response = s3_client.get_object(Bucket=bucket, Key=prompt_key)\n",
    "    video_prompt = response['Body'].read().decode('utf-8')\n",
    "    print(f\"üìù Video prompt: {video_prompt}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error reading prompt file: {e}\")\n",
    "    video_prompt = \"A cat playing with a ball of yarn in a cozy living room\"  # Fallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Q&A pairs for subject alignment\n",
    "focus_area = \"background_alignment\"\n",
    "print(f\"üéØ Generating Q&A pairs for: {focus_area}\")\n",
    "\n",
    "qa_data = generate_qa_alignment(\n",
    "    boto3_session=session,\n",
    "    video_prompt=video_prompt,\n",
    "    focus_area=focus_area,\n",
    "    model_id=MODEL_ID\n",
    ")\n",
    "\n",
    "if qa_data:\n",
    "    print(f\"\\nüìã Generated {len(qa_data.get('questions', []))} questions\")\n",
    "    print(f\"üß© Atomic tuples: {json.dumps(qa_data.get('atomic_tuples', []), indent = 2)}\")\n",
    "    \n",
    "    # Display first question as example\n",
    "    if qa_data.get('questions'):\n",
    "        first_q = qa_data['questions'][0]\n",
    "        print(f\"\\n‚ùì Example Question: {first_q['question']}\")\n",
    "        print(f\"üìù Answer Choices: {first_q['answer_choices']}\")\n",
    "        print(f\"‚úÖ Correct Answer: {first_q['correct_answer'][0]}\")\n",
    "\n",
    "        sec_q = qa_data['questions'][1]\n",
    "        print(f\"\\n‚ùì Example Question: {sec_q['question']}\")\n",
    "        print(f\"üìù Answer Choices: {sec_q['answer_choices']}\")\n",
    "        print(f\"‚úÖ Correct Answer: {sec_q['correct_answer'][0]}\")\n",
    "else:\n",
    "    print(\"‚ùå Failed to generate Q&A pairs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F. Step 2 - Video Analysis with Multimodal AI\n",
    "\n",
    "Now we use Amazon Nova Premier's multimodal capabilities to analyze the actual video content and answer our generated questions. The `evaluate_video_qa` function:\n",
    "\n",
    "**Key Operations:**\n",
    "1. **Multimodal input** - Processes both video content and text questions simultaneously\n",
    "2. **Visual analysis** - Examines video frames to understand content\n",
    "3. **Answer selection** - Chooses from provided multiple choice options\n",
    "4. **Fallback handling** - Returns \"None\" when uncertain\n",
    "\n",
    "**Why this matters:** This step bridges the gap between text descriptions and actual video content, enabling objective measurement of how well the generated video matches the intended prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test video analysis with the first question\n",
    "if qa_data and qa_data.get('questions'):\n",
    "    test_question = qa_data['questions'][0]\n",
    "    question = test_question['question']\n",
    "    choices = test_question['answer_choices']\n",
    "    correct_answer = test_question['correct_answer'][0]\n",
    "    \n",
    "    print(f\"üé¨ Analyzing video with question: {question}\")\n",
    "    print(f\"üìã Choices: {choices}\")\n",
    "    \n",
    "    model_answer = evaluate_video_qa(\n",
    "        session,\n",
    "        s3_video_uri=S3_VIDEO_URI,\n",
    "        question=question,\n",
    "        answer_choices=choices,\n",
    "        model_id=MODEL_ID\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nü§ñ Model's answer: {model_answer}\")\n",
    "    print(f\"‚úÖ Correct answer: {correct_answer}\")\n",
    "    print(f\"üéØ Match: {'‚úÖ Yes' if model_answer.strip().lower() == correct_answer.lower() else '‚ùå No'}\")\n",
    "else:\n",
    "    print(\"‚ùå No questions available for testing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## G. Step 3 - Calculate Alignment Scores\n",
    "\n",
    "This step evaluates all questions for a specific focus area and calculates an alignment score. We iterate through each question, get the model's answer, and compare it with the correct answer to generate a numerical score.\n",
    "\n",
    "**Key Operations:**\n",
    "1. **Question iteration** - Process all 5 questions for the focus area\n",
    "2. **Answer comparison** - Match model responses with correct answers\n",
    "3. **Score calculation** - Count correct answers out of total questions\n",
    "4. **Progress tracking** - Show evaluation progress\n",
    "\n",
    "**Scoring:** Each focus area receives a score from 0-5, representing the number of correctly answered questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate alignment score for the focus area\n",
    "if qa_data and qa_data.get('questions'):\n",
    "    questions = qa_data['questions']\n",
    "    score = 0\n",
    "    results = []\n",
    "    \n",
    "    print(f\"üìä Evaluating {len(questions)} questions for {focus_area}...\")\n",
    "    \n",
    "    for i, q_data in enumerate(questions):\n",
    "        question = q_data['question']\n",
    "        answer_choices = q_data['answer_choices']\n",
    "        correct_answer = q_data['correct_answer'][0]\n",
    "        \n",
    "        print(f\"\\n‚ùì Question {i+1}: {question}\")\n",
    "        \n",
    "        # Get model's answer\n",
    "        model_answer = evaluate_video_qa(\n",
    "            session,\n",
    "            S3_VIDEO_URI,\n",
    "            question=question,\n",
    "            answer_choices=answer_choices\n",
    "        )\n",
    "        \n",
    "        # Check if answer is correct\n",
    "        is_correct = model_answer.strip().lower() == correct_answer.lower()\n",
    "        if is_correct:\n",
    "            score += 1\n",
    "            \n",
    "        results.append({\n",
    "            'question': question,\n",
    "            'model_answer': model_answer,\n",
    "            'correct_answer': correct_answer,\n",
    "            'is_correct': is_correct\n",
    "        })\n",
    "        \n",
    "        print(f\"ü§ñ Model: {model_answer}\")\n",
    "        print(f\"‚úÖ Correct: {correct_answer}\")\n",
    "        print(f\"üéØ Result: {'‚úÖ Correct' if is_correct else '‚ùå Incorrect'}\")\n",
    "    \n",
    "    print(f\"\\nüìà Final Score for {focus_area}: {score}/{len(questions)} ({score/len(questions)*100:.1f}%)\")\n",
    "else:\n",
    "    print(\"‚ùå No questions available for scoring\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## H. Complete Evaluation Pipeline\n",
    "\n",
    "The `evaluation_pipeline` function demonstrates the complete evaluation workflow across all focus areas. This wrapper function:\n",
    "\n",
    "**Key Operations:**\n",
    "1. **Multi-focus evaluation** - Processes all 5 focus areas automatically\n",
    "2. **S3 integration** - Reads prompts and saves results to S3\n",
    "3. **Data persistence** - Stores both scores and Q&A data for analysis\n",
    "4. **Error handling** - Gracefully handles missing files or API failures\n",
    "\n",
    "**Output Structure:**\n",
    "```\n",
    "s3://<bucket>/\n",
    "‚îî‚îÄ‚îÄ generated_videos/\n",
    "    ‚îú‚îÄ‚îÄ <video_id>.mp4\n",
    "    ‚îú‚îÄ‚îÄ <video_id>_prompt.txt\n",
    "    ‚îú‚îÄ‚îÄ <video_id>_alignment.json      # Alignment scores\n",
    "    ‚îî‚îÄ‚îÄ <video_id>_alignment_qa.json   # Q&A data\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîß Using production pipeline from utils/vid_eval.py...\")\n",
    "\n",
    "# Run the production pipeline\n",
    "production_results = evaluation_pipeline(\n",
    "    s3_video_uri=S3_VIDEO_URI,\n",
    "    boto3_session=session,\n",
    "    model_id=MODEL_ID\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Production pipeline completed!\")\n",
    "print(f\"üìä Results: {json.dumps(production_results, indent=2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "You've successfully implemented a comprehensive video evaluation system that:\n",
    "\n",
    "1. **Generates structured Q&A pairs** based on video prompts and focus areas\n",
    "2. **Analyzes videos using multimodal AI** to answer questions about visual content\n",
    "3. **Calculates alignment scores** across multiple evaluation dimensions\n",
    "\n",
    "Now you can move to the next module to see how to evaluate the quality of the generated video"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
