{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<small>Copyright 2025 Amazon.com, Inc. or its affiliates. All Rights Reserved.<br>\n",
    "This is AWS Content subject to the terms of the Customer Agreement</small>\n",
    "\n",
    "# Module 4: End-to-End ROBO-Reviewer Pipeline\n",
    "\n",
    "Welcome to the complete ROBO-Reviewer system! This notebook demonstrates the full automated pipeline that takes your generated videos and provides comprehensive evaluation reports.\n",
    "\n",
    "## What This Pipeline Does\n",
    "\n",
    "**Input**: Generated videos with their prompts  \n",
    "**Output**: Comprehensive evaluation reports with scores and insights\n",
    "\n",
    "**Automated Process**:\n",
    "1. üé¨ **Video Discovery**: Finds your generated videos and prompts\n",
    "2. üñºÔ∏è **Frame Sampling**: Intelligently extracts representative frames\n",
    "3. ‚ùì **Content Alignment**: Evaluates prompt-video alignment using Q&A\n",
    "4. ‚≠ê **Quality Assessment**: Scores video quality using LLM-as-Judge\n",
    "5. üìä **Report Generation**: Creates comprehensive evaluation reports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Dependencies\n",
    "\n",
    "First, let's install the required packages for this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q matplotlib opencv-python Pillow tqdm pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "from utils.content_alignment import *\n",
    "from utils.quality_assessment import *\n",
    "from utils.video_processing import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AWS Configuration\n",
    "session = boto3.Session()\n",
    "s3_client = session.client('s3')\n",
    "\n",
    "# Import S3 bucket configuration utility\n",
    "from utils.config import get_s3_bucket\n",
    "\n",
    "# Get S3 bucket name\n",
    "S3_BUCKET = get_s3_bucket(session)\n",
    "\n",
    "# Load configuration for video prefix\n",
    "with open('config.json', 'r') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "VIDEO_PREFIX = config['video_prefix']\n",
    "\n",
    "# Evaluation Configuration\n",
    "MODEL_ID = \"us.amazon.nova-premier-v1:0\"\n",
    "\n",
    "# Focus areas for Q&A evaluation\n",
    "FOCUS_AREAS = [\n",
    "    \"subject_alignment\",\n",
    "    \"background_alignment\", \n",
    "    \"color_accuracy\",\n",
    "    \"activity_alignment\",\n",
    "    \"spatial_relationships\"\n",
    "]\n",
    "\n",
    "print(\"üöÄ ROBO-Reviewer Pipeline Initialized\")\n",
    "print(f\"üìÅ S3 Bucket: {S3_BUCKET}\")\n",
    "print(f\"üé¨ Video Location: {VIDEO_PREFIX}\")\n",
    "print(f\"ü§ñ Model: {MODEL_ID}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Video Discovery\n",
    "\n",
    "First, let's discover all the videos and their corresponding prompts in your S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discover_videos_and_prompts(bucket_name, prefix):\n",
    "    \"\"\"Discover video files and their corresponding prompt files\"\"\"\n",
    "    \n",
    "    try:\n",
    "        # List all objects in the bucket with the given prefix\n",
    "        response = s3_client.list_objects_v2(Bucket=bucket_name, Prefix=prefix)\n",
    "        \n",
    "        if 'Contents' not in response:\n",
    "            print(f\"‚ùå No files found in s3://{bucket_name}/{prefix}\")\n",
    "            return []\n",
    "        \n",
    "        # Find video files\n",
    "        video_files = []\n",
    "        for obj in response['Contents']:\n",
    "            key = obj['Key']\n",
    "            if key.endswith('.mp4'):\n",
    "                video_uri = f\"s3://{bucket_name}/{key}\"\n",
    "                prompt_key = key.replace('.mp4', '_prompt.txt')\n",
    "                \n",
    "                # Check if corresponding prompt file exists\n",
    "                try:\n",
    "                    prompt_response = s3_client.get_object(Bucket=bucket_name, Key=prompt_key)\n",
    "                    prompt_text = prompt_response['Body'].read().decode('utf-8')\n",
    "                    \n",
    "                    video_files.append({\n",
    "                        'video_uri': video_uri,\n",
    "                        'prompt': prompt_text,\n",
    "                        'video_name': key.split('/')[-1].replace('.mp4', '')\n",
    "                    })\n",
    "                except:\n",
    "                    print(f\"‚ö†Ô∏è  Prompt file not found for {key}, skipping...\")\n",
    "        \n",
    "        return video_files\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error discovering videos: {e}\")\n",
    "        return []\n",
    "\n",
    "# Discover videos\n",
    "print(\"üîç Discovering videos and prompts...\")\n",
    "video_data = discover_videos_and_prompts(S3_BUCKET, VIDEO_PREFIX)\n",
    "\n",
    "if video_data:\n",
    "    print(f\"‚úÖ Found {len(video_data)} video(s) with prompts:\")\n",
    "    for i, data in enumerate(video_data, 1):\n",
    "        print(f\"   {i}. {data['video_name']}\")\n",
    "        print(f\"      Prompt: {data['prompt'][:100]}{'...' if len(data['prompt']) > 100 else ''}\")\n",
    "        print()\n",
    "else:\n",
    "    print(\"‚ùå No videos found. Please check your S3 bucket and prefix configuration.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Automated Evaluation Pipeline\n",
    "\n",
    "Now let's run the complete evaluation pipeline for each discovered video. This will:\n",
    "- Generate Q&A pairs for content alignment\n",
    "- Evaluate video quality using LLM-as-Judge\n",
    "- Save results to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_complete_evaluation(video_info):\n",
    "    \"\"\"Run both content alignment and quality evaluation for a video\"\"\"\n",
    "    \n",
    "    video_uri = video_info['video_uri']\n",
    "    video_name = video_info['video_name']\n",
    "    prompt = video_info['prompt']\n",
    "    \n",
    "    print(f\"\\nüé¨ Evaluating: {video_name}\")\n",
    "    print(f\"üìù Prompt: {prompt}\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    results = {\n",
    "        'video_name': video_name,\n",
    "        'video_uri': video_uri,\n",
    "        'prompt': prompt,\n",
    "        'evaluation_timestamp': datetime.now().isoformat()\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # 1. Content Alignment Evaluation (Q&A)\n",
    "        print(\"\\n‚ùì Running Content Alignment Evaluation...\")\n",
    "        alignment_results = evaluation_pipeline(\n",
    "            s3_video_uri=video_uri,\n",
    "            boto3_session=session,\n",
    "            model_id=MODEL_ID,\n",
    "            focus_areas=FOCUS_AREAS\n",
    "        )\n",
    "        \n",
    "        if alignment_results and video_uri in alignment_results:\n",
    "            results['content_alignment'] = alignment_results[video_uri]\n",
    "            \n",
    "            # Calculate overall alignment score\n",
    "            total_score = sum(alignment_results[video_uri].values())\n",
    "            max_score = len(FOCUS_AREAS) * 5  # 5 questions per focus area\n",
    "            alignment_percentage = (total_score / max_score) * 100\n",
    "            results['alignment_score'] = alignment_percentage\n",
    "            \n",
    "            print(f\"‚úÖ Content Alignment: {alignment_percentage:.1f}% ({total_score}/{max_score})\")\n",
    "        else:\n",
    "            print(\"‚ùå Content alignment evaluation failed\")\n",
    "            results['content_alignment'] = {}\n",
    "            results['alignment_score'] = 0\n",
    "        \n",
    "        # 2. Quality Assessment (LLM-as-Judge)\n",
    "        print(\"\\n‚≠ê Running Quality Assessment...\")\n",
    "        quality_results = video_quality_evaluation_pipeline(\n",
    "            s3_video_uri=video_uri,\n",
    "            boto3_session=session,\n",
    "            model_id=MODEL_ID,\n",
    "            temporal_consistency_flag=True,\n",
    "            aesthetic_quality_flag=True,\n",
    "            technical_quality_flag=True,\n",
    "            motion_effects_flag=True\n",
    "        )\n",
    "        \n",
    "        if quality_results:\n",
    "            results['quality_assessment'] = quality_results\n",
    "            \n",
    "            # Calculate overall quality score\n",
    "            quality_scores = [metrics['score'] for metrics in quality_results.values()]\n",
    "            avg_quality = sum(quality_scores) / len(quality_scores)\n",
    "            results['quality_score'] = avg_quality\n",
    "            \n",
    "            print(f\"‚úÖ Quality Score: {avg_quality:.1f}/5.0\")\n",
    "            for metric, data in quality_results.items():\n",
    "                print(f\"   {metric.replace('_', ' ').title()}: {data['score']}/5\")\n",
    "        else:\n",
    "            print(\"‚ùå Quality assessment failed\")\n",
    "            results['quality_assessment'] = {}\n",
    "            results['quality_score'] = 0\n",
    "        \n",
    "        print(f\"\\nüéØ Overall Evaluation Complete for {video_name}\")\n",
    "        return results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error evaluating {video_name}: {e}\")\n",
    "        results['error'] = str(e)\n",
    "        return results\n",
    "\n",
    "# Run evaluation for all discovered videos\n",
    "if video_data:\n",
    "    print(\"üöÄ Starting Complete Evaluation Pipeline...\")\n",
    "    print(f\"üìä Will evaluate {len(video_data)} video(s)\")\n",
    "    \n",
    "    all_results = []\n",
    "    \n",
    "    for i, video_info in enumerate(video_data, 1):\n",
    "        print(f\"\\n{'='*20} Video {i}/{len(video_data)} {'='*20}\")\n",
    "        result = run_complete_evaluation(video_info)\n",
    "        all_results.append(result)\n",
    "        \n",
    "        # Add a small delay between evaluations\n",
    "        if i < len(video_data):\n",
    "            print(\"\\n‚è≥ Waiting 5 seconds before next evaluation...\")\n",
    "            time.sleep(5)\n",
    "    \n",
    "    print(\"\\nüéâ All evaluations complete!\")\n",
    "else:\n",
    "    print(\"‚ùå No videos to evaluate\")\n",
    "    all_results = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Generate Comprehensive Report\n",
    "\n",
    "Let's create a comprehensive HTML report with all evaluation results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_evaluation_report(results_list):\n",
    "    \"\"\"Generate a comprehensive HTML evaluation report\"\"\"\n",
    "    \n",
    "    if not results_list:\n",
    "        return \"<h2>No evaluation results to display</h2>\"\n",
    "    \n",
    "    # Calculate summary statistics\n",
    "    valid_results = [r for r in results_list if 'error' not in r]\n",
    "    \n",
    "    if not valid_results:\n",
    "        return \"<h2>No valid evaluation results</h2>\"\n",
    "    \n",
    "    avg_alignment = sum(r.get('alignment_score', 0) for r in valid_results) / len(valid_results)\n",
    "    avg_quality = sum(r.get('quality_score', 0) for r in valid_results) / len(valid_results)\n",
    "    \n",
    "    # Start building HTML report\n",
    "    html = f\"\"\"\n",
    "    <style>\n",
    "        .report-container {{ font-family: Arial, sans-serif; max-width: 1200px; margin: 0 auto; }}\n",
    "        .header {{ background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: white; padding: 20px; border-radius: 10px; margin-bottom: 20px; }}\n",
    "        .summary {{ background: #f8f9fa; padding: 15px; border-radius: 8px; margin-bottom: 20px; }}\n",
    "        .video-card {{ border: 1px solid #ddd; border-radius: 8px; margin-bottom: 20px; overflow: hidden; }}\n",
    "        .video-header {{ background: #343a40; color: white; padding: 15px; }}\n",
    "        .video-content {{ padding: 15px; }}\n",
    "        .score-grid {{ display: grid; grid-template-columns: repeat(auto-fit, minmax(200px, 1fr)); gap: 15px; margin: 15px 0; }}\n",
    "        .score-card {{ background: #e9ecef; padding: 10px; border-radius: 5px; text-align: center; }}\n",
    "        .score-high {{ background: #d4edda; color: #155724; }}\n",
    "        .score-medium {{ background: #fff3cd; color: #856404; }}\n",
    "        .score-low {{ background: #f8d7da; color: #721c24; }}\n",
    "        .prompt-box {{ background: #f1f3f4; padding: 10px; border-left: 4px solid #4285f4; margin: 10px 0; }}\n",
    "        .metric-details {{ margin-top: 15px; }}\n",
    "        .metric-item {{ margin: 8px 0; padding: 8px; background: #f8f9fa; border-radius: 4px; }}\n",
    "    </style>\n",
    "    \n",
    "    <div class=\"report-container\">\n",
    "        <div class=\"header\">\n",
    "            <h1>ü§ñ ROBO-Reviewer Evaluation Report</h1>\n",
    "            <p>Generated on {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>\n",
    "        </div>\n",
    "        \n",
    "        <div class=\"summary\">\n",
    "            <h2>üìä Summary Statistics</h2>\n",
    "            <div class=\"score-grid\">\n",
    "                <div class=\"score-card\">\n",
    "                    <h3>Videos Evaluated</h3>\n",
    "                    <h2>{len(valid_results)}</h2>\n",
    "                </div>\n",
    "                <div class=\"score-card {'score-high' if avg_alignment >= 80 else 'score-medium' if avg_alignment >= 60 else 'score-low'}\">\n",
    "                    <h3>Avg Content Alignment</h3>\n",
    "                    <h2>{avg_alignment:.1f}%</h2>\n",
    "                </div>\n",
    "                <div class=\"score-card {'score-high' if avg_quality >= 4 else 'score-medium' if avg_quality >= 3 else 'score-low'}\">\n",
    "                    <h3>Avg Quality Score</h3>\n",
    "                    <h2>{avg_quality:.1f}/5.0</h2>\n",
    "                </div>\n",
    "            </div>\n",
    "        </div>\n",
    "    \"\"\"\n",
    "    \n",
    "    # Add individual video results\n",
    "    for i, result in enumerate(valid_results, 1):\n",
    "        alignment_score = result.get('alignment_score', 0)\n",
    "        quality_score = result.get('quality_score', 0)\n",
    "        \n",
    "        # Determine score classes\n",
    "        alignment_class = 'score-high' if alignment_score >= 80 else 'score-medium' if alignment_score >= 60 else 'score-low'\n",
    "        quality_class = 'score-high' if quality_score >= 4 else 'score-medium' if quality_score >= 3 else 'score-low'\n",
    "        \n",
    "        html += f\"\"\"\n",
    "        <div class=\"video-card\">\n",
    "            <div class=\"video-header\">\n",
    "                <h2>üé¨ Video {i}: {result['video_name']}</h2>\n",
    "            </div>\n",
    "            <div class=\"video-content\">\n",
    "                <div class=\"prompt-box\">\n",
    "                    <strong>üìù Original Prompt:</strong><br>\n",
    "                    {result['prompt']}\n",
    "                </div>\n",
    "                \n",
    "                <div class=\"score-grid\">\n",
    "                    <div class=\"score-card {alignment_class}\">\n",
    "                        <h3>Content Alignment</h3>\n",
    "                        <h2>{alignment_score:.1f}%</h2>\n",
    "                    </div>\n",
    "                    <div class=\"score-card {quality_class}\">\n",
    "                        <h3>Quality Score</h3>\n",
    "                        <h2>{quality_score:.1f}/5.0</h2>\n",
    "                    </div>\n",
    "                </div>\n",
    "        \"\"\"\n",
    "        \n",
    "        # Add content alignment details\n",
    "        if 'content_alignment' in result and result['content_alignment']:\n",
    "            html += \"<div class='metric-details'><h4>‚ùì Content Alignment Details:</h4>\"\n",
    "            for focus_area, score in result['content_alignment'].items():\n",
    "                html += f\"<div class='metric-item'><strong>{focus_area.title()}:</strong> {score}/5</div>\"\n",
    "            html += \"</div>\"\n",
    "        \n",
    "        # Add quality assessment details\n",
    "        if 'quality_assessment' in result and result['quality_assessment']:\n",
    "            html += \"<div class='metric-details'><h4>‚≠ê Quality Assessment Details:</h4>\"\n",
    "            for metric, data in result['quality_assessment'].items():\n",
    "                metric_name = metric.replace('_', ' ').title()\n",
    "                html += f\"\"\"\n",
    "                <div class='metric-item'>\n",
    "                    <strong>{metric_name}:</strong> {data['score']}/5<br>\n",
    "                    <small><em>{data.get('justification', 'No justification provided')}</em></small>\n",
    "                </div>\n",
    "                \"\"\"\n",
    "            html += \"</div>\"\n",
    "        \n",
    "        html += \"</div></div>\"  # Close video-content and video-card\n",
    "    \n",
    "    html += \"</div>\"  # Close report-container\n",
    "    \n",
    "    return html\n",
    "\n",
    "# Generate and display the report\n",
    "if all_results:\n",
    "    print(\"üìä Generating comprehensive evaluation report...\")\n",
    "    report_html = generate_evaluation_report(all_results)\n",
    "    \n",
    "    # Display the report\n",
    "    display(HTML(report_html))\n",
    "    \n",
    "    # Create final_report directory if it doesn't exist\n",
    "    import os\n",
    "    os.makedirs('final_report', exist_ok=True)\n",
    "    \n",
    "    # Save report to file\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    report_filename = f\"final_report/robo_reviewer_report_{timestamp}.html\"\n",
    "    \n",
    "    with open(report_filename, 'w', encoding='utf-8') as f:\n",
    "        f.write(report_html)\n",
    "    \n",
    "    print(f\"\\nüíæ Report saved as: {report_filename}\")\n",
    "else:\n",
    "    print(\"‚ùå No results to generate report\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Export Results for Further Analysis\n",
    "\n",
    "Let's also create a structured data export for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a structured DataFrame for analysis\n",
    "if all_results:\n",
    "    # Prepare data for DataFrame\n",
    "    df_data = []\n",
    "    \n",
    "    for result in all_results:\n",
    "        if 'error' not in result:\n",
    "            row = {\n",
    "                'video_name': result['video_name'],\n",
    "                'prompt': result['prompt'],\n",
    "                'alignment_score': result.get('alignment_score', 0),\n",
    "                'quality_score': result.get('quality_score', 0),\n",
    "                'evaluation_timestamp': result['evaluation_timestamp']\n",
    "            }\n",
    "            \n",
    "            # Add content alignment scores\n",
    "            if 'content_alignment' in result:\n",
    "                for focus_area, score in result['content_alignment'].items():\n",
    "                    row[f'alignment_{focus_area.replace(\" \", \"_\")}'] = score\n",
    "            \n",
    "            # Add quality scores\n",
    "            if 'quality_assessment' in result:\n",
    "                for metric, data in result['quality_assessment'].items():\n",
    "                    row[f'quality_{metric}'] = data['score']\n",
    "            \n",
    "            df_data.append(row)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    if df_data:\n",
    "        df = pd.DataFrame(df_data)\n",
    "        \n",
    "        print(\"üìà Evaluation Results Summary:\")\n",
    "        print(\"=\" * 50)\n",
    "        display(df[['video_name', 'alignment_score', 'quality_score']].round(2))\n",
    "        \n",
    "        # Save to CSV in final_report folder\n",
    "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        csv_filename = f\"final_report/robo_reviewer_results_{timestamp}.csv\"\n",
    "        df.to_csv(csv_filename, index=False)\n",
    "        \n",
    "        print(f\"\\nüíæ Results exported to: {csv_filename}\")\n",
    "        \n",
    "        # Save detailed results as JSON in final_report folder\n",
    "        json_filename = f\"final_report/robo_reviewer_detailed_{timestamp}.json\"\n",
    "        with open(json_filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(all_results, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        print(f\"üíæ Detailed results saved as: {json_filename}\")\n",
    "    else:\n",
    "        print(\"‚ùå No valid results to export\")\n",
    "else:\n",
    "    print(\"‚ùå No results to export\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Insights and Recommendations\n",
    "\n",
    "Based on the evaluation results, let's generate some insights about Nova Reel's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_insights(results_list):\n",
    "    \"\"\"Generate insights and recommendations based on evaluation results\"\"\"\n",
    "    \n",
    "    valid_results = [r for r in results_list if 'error' not in r and r.get('alignment_score', 0) > 0]\n",
    "    \n",
    "    if not valid_results:\n",
    "        return \"No valid results for analysis\"\n",
    "    \n",
    "    insights = []\n",
    "    \n",
    "    # Overall performance analysis\n",
    "    avg_alignment = sum(r.get('alignment_score', 0) for r in valid_results) / len(valid_results)\n",
    "    avg_quality = sum(r.get('quality_score', 0) for r in valid_results) / len(valid_results)\n",
    "    \n",
    "    insights.append(f\"üìä **Overall Performance Analysis**\")\n",
    "    insights.append(f\"   ‚Ä¢ Average Content Alignment: {avg_alignment:.1f}%\")\n",
    "    insights.append(f\"   ‚Ä¢ Average Quality Score: {avg_quality:.1f}/5.0\")\n",
    "    \n",
    "    # Performance categorization\n",
    "    if avg_alignment >= 80:\n",
    "        insights.append(f\"   ‚Ä¢ ‚úÖ Excellent content alignment - Nova Reel is following prompts very well\")\n",
    "    elif avg_alignment >= 60:\n",
    "        insights.append(f\"   ‚Ä¢ ‚ö†Ô∏è  Good content alignment - Some room for prompt optimization\")\n",
    "    else:\n",
    "        insights.append(f\"   ‚Ä¢ ‚ùå Content alignment needs improvement - Consider refining prompts\")\n",
    "    \n",
    "    if avg_quality >= 4:\n",
    "        insights.append(f\"   ‚Ä¢ ‚úÖ High quality video generation\")\n",
    "    elif avg_quality >= 3:\n",
    "        insights.append(f\"   ‚Ä¢ ‚ö†Ô∏è  Moderate quality - Some technical aspects could be improved\")\n",
    "    else:\n",
    "        insights.append(f\"   ‚Ä¢ ‚ùå Quality concerns - May need different generation parameters\")\n",
    "    \n",
    "    # Focus area analysis\n",
    "    if len(valid_results) > 0 and 'content_alignment' in valid_results[0]:\n",
    "        focus_scores = {}\n",
    "        for focus_area in FOCUS_AREAS:\n",
    "            scores = [r['content_alignment'].get(focus_area, 0) for r in valid_results if 'content_alignment' in r]\n",
    "            if scores:\n",
    "                focus_scores[focus_area] = sum(scores) / len(scores)\n",
    "        \n",
    "        if focus_scores:\n",
    "            insights.append(f\"\\nüéØ **Focus Area Performance**\")\n",
    "            sorted_areas = sorted(focus_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "            \n",
    "            best_area = sorted_areas[0]\n",
    "            worst_area = sorted_areas[-1]\n",
    "            \n",
    "            insights.append(f\"   ‚Ä¢ üèÜ Strongest: {best_area[0].title()} ({best_area[1]:.1f}/5.0)\")\n",
    "            insights.append(f\"   ‚Ä¢ üìà Needs work: {worst_area[0].title()} ({worst_area[1]:.1f}/5.0)\")\n",
    "    \n",
    "    # Quality metrics analysis\n",
    "    if len(valid_results) > 0 and 'quality_assessment' in valid_results[0]:\n",
    "        quality_metrics = {}\n",
    "        for result in valid_results:\n",
    "            if 'quality_assessment' in result:\n",
    "                for metric, data in result['quality_assessment'].items():\n",
    "                    if metric not in quality_metrics:\n",
    "                        quality_metrics[metric] = []\n",
    "                    quality_metrics[metric].append(data['score'])\n",
    "        \n",
    "        if quality_metrics:\n",
    "            insights.append(f\"\\n‚≠ê **Quality Metrics Performance**\")\n",
    "            for metric, scores in quality_metrics.items():\n",
    "                avg_score = sum(scores) / len(scores)\n",
    "                metric_name = metric.replace('_', ' ').title()\n",
    "                status = \"‚úÖ\" if avg_score >= 4 else \"‚ö†Ô∏è\" if avg_score >= 3 else \"‚ùå\"\n",
    "                insights.append(f\"   ‚Ä¢ {status} {metric_name}: {avg_score:.1f}/5.0\")\n",
    "    \n",
    "    # Recommendations\n",
    "    insights.append(f\"\\nüí° **Recommendations**\")\n",
    "    \n",
    "    if avg_alignment < 70:\n",
    "        insights.append(f\"   ‚Ä¢ Consider more specific and detailed prompts\")\n",
    "        insights.append(f\"   ‚Ä¢ Add technical specifications (4k, cinematic, etc.)\")\n",
    "        insights.append(f\"   ‚Ä¢ Include camera movement descriptions\")\n",
    "    \n",
    "    if avg_quality < 3.5:\n",
    "        insights.append(f\"   ‚Ä¢ Experiment with different seeds for better quality\")\n",
    "        insights.append(f\"   ‚Ä¢ Try shorter, more focused prompts\")\n",
    "        insights.append(f\"   ‚Ä¢ Consider adjusting video generation parameters\")\n",
    "    \n",
    "    insights.append(f\"   ‚Ä¢ Use this evaluation data to iterate and improve prompts\")\n",
    "    insights.append(f\"   ‚Ä¢ Focus on improving the lowest-scoring focus areas\")\n",
    "    \n",
    "    return \"\\n\".join(insights)\n",
    "\n",
    "# Generate and display insights\n",
    "if all_results:\n",
    "    print(\"üß† Generating Insights and Recommendations...\")\n",
    "    print(\"=\" * 60)\n",
    "    insights = generate_insights(all_results)\n",
    "    print(insights)\n",
    "else:\n",
    "    print(\"‚ùå No results available for insights generation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "üéâ **Congratulations!** You've successfully completed the end-to-end ROBO-Reviewer pipeline!\n",
    "\n",
    "### What You've Accomplished:\n",
    "\n",
    "‚úÖ **Automated Video Discovery**: Found and processed videos with their prompts  \n",
    "‚úÖ **Content Alignment Evaluation**: Measured how well videos match their prompts  \n",
    "‚úÖ **Quality Assessment**: Evaluated technical and aesthetic video quality  \n",
    "‚úÖ **Comprehensive Reporting**: Generated detailed HTML and data reports  \n",
    "‚úÖ **Actionable Insights**: Received recommendations for improvement  \n",
    "\n",
    "### Files Generated:\n",
    "- üìä **HTML Report**: Visual evaluation report with scores and details\n",
    "- üìà **CSV Export**: Structured data for further analysis\n",
    "- üìã **JSON Details**: Complete evaluation results with all metadata\n",
    "\n",
    "### Next Steps:\n",
    "1. **Analyze Results**: Use the insights to understand Nova Reel's strengths and limitations\n",
    "2. **Optimize Prompts**: Apply recommendations to improve future video generation\n",
    "3. **Scale Up**: Use this pipeline to evaluate larger batches of videos\n",
    "4. **Customize**: Modify evaluation criteria for your specific use cases\n",
    "\n",
    "### The Power of ROBO-Reviewer:\n",
    "You now have an automated system that can:\n",
    "- Process hundreds of videos without manual intervention\n",
    "- Provide objective, consistent evaluation criteria\n",
    "- Generate professional reports for stakeholders\n",
    "- Identify patterns and improvement opportunities\n",
    "- Scale video evaluation to enterprise levels\n",
    "\n",
    "**This is the future of AI video evaluation - automated, objective, and scalable!** üöÄ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
